{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-md",
   "metadata": {},
   "source": [
    "# Data Quality Analysis & Cleaning\n",
    "\n",
    "Dieses Notebook widmet sich der systematischen Prüfung und Bereinigung der Datenqualität.\n",
    "Basierend auf den Erkenntnissen der EDA werden hier Maßnahmen ergriffen, um einen sauberen Datensatz für nachfolgende Analysen zu erstellen.\n",
    "\n",
    "**Ziele:**\n",
    "- Behandlung fehlender Werte (Missing Values)\n",
    "- Entfernung von Duplikaten\n",
    "- Bereinigung inkonsistenter Daten (z.B. negative Mengen, Preise <= 0)\n",
    "- Speichern des bereinigten Datensatzes als `retail_clean.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:43:56.503207Z",
     "iopub.status.busy": "2026-01-27T12:43:56.503207Z",
     "iopub.status.idle": "2026-01-27T12:43:56.852896Z",
     "shell.execute_reply": "2026-01-27T12:43:56.852896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Daten von: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\data-science-projects\\1.1-ecommerce-analytics\\data\\processed\\retail_raw.parquet\n",
      "Original Shape: (541909, 8)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pfade definieren\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"retail_raw.parquet\"\n",
    "OUTPUT_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"retail_clean.parquet\"\n",
    "\n",
    "print(\"Lade Daten von:\", DATA_PATH)\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(\"Original Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-md",
   "metadata": {},
   "source": [
    "## 1. Missing Values\n",
    "\n",
    "Wir prüfen auf fehlende Werte. Ein kritischer Punkt ist die `CustomerID`.\n",
    "Zeilen ohne `CustomerID` können nicht für kundenbezogene Analysen (CLV, Cohorts) genutzt werden, sind aber für die reine Umsatzbetrachtung relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "missing-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:43:56.852896Z",
     "iopub.status.busy": "2026-01-27T12:43:56.852896Z",
     "iopub.status.idle": "2026-01-27T12:43:56.866094Z",
     "shell.execute_reply": "2026-01-27T12:43:56.866094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Werte pro Spalte:\n",
      "CustomerID    135080\n",
      "dtype: int64\n",
      "Missing Values nach Imputation in Description: 0\n"
     ]
    }
   ],
   "source": [
    "missing_counts = df.isna().sum()\n",
    "print(\"Fehlende Werte pro Spalte:\")\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Entscheidung: Wir behalten Zeilen ohne CustomerID vorerst, markieren sie aber.\n",
    "# Für spezifische Kundenanalysen müssen diese später gefiltert werden.\n",
    "# Wir füllen NaN in Description mit 'Unknown'\n",
    "\n",
    "df['Description'] = df['Description'].fillna('Unknown')\n",
    "print(\"Missing Values nach Imputation in Description:\", df['Description'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicates-md",
   "metadata": {},
   "source": [
    "## 2. Duplikate\n",
    "\n",
    "Vollständige Duplikate verzerren die Analyse und sollten entfernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "duplicates-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:43:56.866094Z",
     "iopub.status.busy": "2026-01-27T12:43:56.866094Z",
     "iopub.status.idle": "2026-01-27T12:43:57.212407Z",
     "shell.execute_reply": "2026-01-27T12:43:57.212407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Duplikate: 5268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplikate entfernt. Neue Shape: (536641, 8)\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Anzahl Duplikate: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Duplikate entfernt. Neue Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inconsistencies-md",
   "metadata": {},
   "source": [
    "## 3. Inkonsistenzen bereinigen\n",
    "\n",
    "- **Negative Mengen**: Oft Retouren oder Stornos. Für die \"Clean Sales\" Analyse entfernen wir diese hier.\n",
    "- **Preise <= 0**: Werbegeschenke oder Fehler. Entfernen.\n",
    "\n",
    "*Hinweis: In einem Rohdaten-Warehouse würden wir diese Daten behalten und flags setzen. Für diesen Analyse-Datensatz filtern wir strikt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inconsistencies-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:43:57.212407Z",
     "iopub.status.busy": "2026-01-27T12:43:57.212407Z",
     "iopub.status.idle": "2026-01-27T12:43:57.259316Z",
     "shell.execute_reply": "2026-01-27T12:43:57.259316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen vor Filterung: 536641\n",
      "Zeilen nach Filterung (Quantity > 0 & Price > 0): 524878\n",
      "Entfernte Zeilen: 11763\n"
     ]
    }
   ],
   "source": [
    "# Filterbedingungen\n",
    "mask_quantity = df['Quantity'] > 0\n",
    "mask_price = df['UnitPrice'] > 0\n",
    "\n",
    "df_clean = df[mask_quantity & mask_price].copy()\n",
    "\n",
    "print(f\"Zeilen vor Filterung: {df.shape[0]}\")\n",
    "print(f\"Zeilen nach Filterung (Quantity > 0 & Price > 0): {df_clean.shape[0]}\")\n",
    "print(f\"Entfernte Zeilen: {df.shape[0] - df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-md",
   "metadata": {},
   "source": [
    "## 4. Speichern\n",
    "\n",
    "Der bereinigte Datensatz wird als `retail_clean.parquet` gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "save-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:43:57.259316Z",
     "iopub.status.busy": "2026-01-27T12:43:57.259316Z",
     "iopub.status.idle": "2026-01-27T12:43:57.435539Z",
     "shell.execute_reply": "2026-01-27T12:43:57.435539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten gespeichert unter: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\data-science-projects\\1.1-ecommerce-analytics\\data\\processed\\retail_clean.parquet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.to_parquet(OUTPUT_PATH)\n",
    "print(f\"Daten gespeichert unter: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecommerce-analytics venv)",
   "language": "python",
   "name": "venv_ecommerce_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
